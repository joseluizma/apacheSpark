{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4ff6810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/03 09:32:22 WARN Utils: Your hostname, exbom-OptiPlex-790, resolves to a loopback address: 127.0.1.1; using 10.101.3.251 instead (on interface enp2s0)\n",
      "25/07/03 09:32:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/07/03 09:32:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexão com o banco de dados bem-sucedida!\n",
      "root\n",
      " |-- c1_101: integer (nullable = true)\n",
      " |-- c2_101: string (nullable = true)\n",
      " |-- c3_101: string (nullable = true)\n",
      " |-- c4_101_5002: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------+-----------+\n",
      "|c1_101|              c2_101|c3_101|c4_101_5002|\n",
      "+------+--------------------+------+-----------+\n",
      "|  1074|     ENTIDADE_FUNÇÃO| t1074|        102|\n",
      "|  1098|       ENTIDADE_TELA| t1098|        102|\n",
      "|  9806|      MENU_SEGURANÇA| t9806|        102|\n",
      "|  9807|MENU_SEGURANÇA_RE...| t9807|        102|\n",
      "|  7406|       CARGO DETALHE| t7406|        102|\n",
      "|  4008|      DESPESA_IMAGEM| t4008|        102|\n",
      "|  4101|PARAMETROS_REGIST...| t4101|        102|\n",
      "|  4102|PARAMETROS_REGIST...| t4102|        102|\n",
      "|  4108|PARAMETROS_REGIST...| t4108|        102|\n",
      "|  7701|          DIAS ÚTEIS| t7701|        102|\n",
      "|  8816|PROCESSO_MOVIMENT...| t8816|        102|\n",
      "|  8802|  PROCESSO_MOVIMENTO| t8802|        102|\n",
      "|  3617|TOTALIZADOR LOG A...| t3617|        102|\n",
      "|  3806|   TABULAÇÃO_DETALHE| t3806|        102|\n",
      "|  7506|POSTO DE TRABALHO...| t7506|        102|\n",
      "|  7507|POSTO DE TRABALHO...| t7507|        102|\n",
      "|  6407|        SETOR FILIAL| t6407|        102|\n",
      "|  3616|LOG AGENDAMENTO M...| t3616|        102|\n",
      "|  8804|  PROCESSO_AUDIÊNCIA| t8804|        102|\n",
      "|  1046|ENTIDADE_CONTATO_...| t1046|        102|\n",
      "+------+--------------------+------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carregar variáveis do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Verifique se o driver JDBC existe\n",
    "jdbc_driver_path = \"/home/repository/spark/02/postgresql-42.7.6.jar\"\n",
    "if not os.path.exists(jdbc_driver_path):\n",
    "    raise FileNotFoundError(f\"Driver JDBC não encontrado em: {jdbc_driver_path}\")\n",
    "\n",
    "# Configurar a sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgreSQL\") \\\n",
    "    .config(\"spark.jars\", jdbc_driver_path) \\\n",
    "    .config(\"spark.driver.extraClassPath\", jdbc_driver_path) \\\n",
    "    .config(\"spark.executor.extraClassPath\", jdbc_driver_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Obter parâmetros de conexão do arquivo .env\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_port = os.getenv(\"DB_PORT\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "# Validar se todas as variáveis foram carregadas\n",
    "if not all([db_host, db_port, db_name, db_user, db_password]):\n",
    "    raise ValueError(\"Uma ou mais variáveis de ambiente não foram definidas no arquivo .env\")\n",
    "\n",
    "# Construir a URL JDBC\n",
    "url = f\"jdbc:postgresql://{db_host}:{db_port}/{db_name}\"\n",
    "\n",
    "# Definir as propriedades de conexão\n",
    "properties = {\n",
    "    \"user\": db_user,\n",
    "    \"password\": db_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Teste a conexão com uma query simples\n",
    "    df_test = spark.read.jdbc(url=url, table=\"(SELECT 1) AS test\", properties=properties)\n",
    "    print(\"Conexão com o banco de dados bem-sucedida!\")\n",
    "    \n",
    "    # Carregue os dados da tabela\n",
    "    df = spark.read.jdbc(url=url, table=\"usu_0.t101\", properties=properties)\n",
    "    \n",
    "    # Imprima o schema e os dados\n",
    "    df.printSchema()\n",
    "    df.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao conectar ou ler a tabela: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    # Feche a sessão Spark\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
