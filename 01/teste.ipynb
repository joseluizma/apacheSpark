{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41217284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-4.0.0.tar.gz (434.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.9\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 KB\u001b[0m \u001b[31m688.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-4.0.0-py2.py3-none-any.whl size=434741258 sha256=5cbceed13aae1c3f6967f4b8c9ff71e34fffa9e0667a6d932fc5ba19448db945\n",
      "  Stored in directory: /home/exbom/.cache/pip/wheels/62/69/eb/eef3014e40bbcff88f1d6dd762baebf6bf5d0266ba57be1ef8\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b397eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e59b8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iniciar um sessão Spark\n",
    "spark = SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "#Leitura do arquivo csv\n",
    "df_vendas = spark.read.csv(\"vendas.csv\", header=True, inferSchema=True)\n",
    "\n",
    "#Leitura do arquivo Json\n",
    "df_clientes = spark.read.json(\"clientes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d788aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equema do DataFrame de vendas:\n",
      "root\n",
      " |-- ID_Venda: integer (nullable = true)\n",
      " |-- Data_Venda: date (nullable = true)\n",
      " |-- Cliente: string (nullable = true)\n",
      " |-- Produto: string (nullable = true)\n",
      " |-- Quantidade: integer (nullable = true)\n",
      " |-- Preco_Unitario: double (nullable = true)\n",
      " |-- Valor_Total: double (nullable = true)\n",
      "\n",
      "\n",
      "Esquema do DataFrame de clientes:\n",
      "root\n",
      " |-- cidade: string (nullable = true)\n",
      " |-- idade: long (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      "\n",
      "Dados de vendas:\n",
      "+--------+----------+------------+-----------+----------+--------------+-----------+\n",
      "|ID_Venda|Data_Venda|     Cliente|    Produto|Quantidade|Preco_Unitario|Valor_Total|\n",
      "+--------+----------+------------+-----------+----------+--------------+-----------+\n",
      "|       1|2024-07-01|  João Silva|   Camiseta|         2|          25.0|       50.0|\n",
      "|       2|2024-07-01| Maria Souza|Calça Jeans|         1|          80.0|       80.0|\n",
      "|       3|2024-07-02|Pedro Santos|      Tênis|         1|         150.0|      150.0|\n",
      "|       4|2024-07-02|   Ana Paula|      Meias|         3|          10.0|       30.0|\n",
      "+--------+----------+------------+-----------+----------+--------------+-----------+\n",
      "\n",
      "\n",
      "Dados de clientes:\n",
      "+--------------+-----+-------------+\n",
      "|        cidade|idade|         nome|\n",
      "+--------------+-----+-------------+\n",
      "|     São Paulo|   30|João da Silva|\n",
      "|Rio de Janeiro|   25|  Maria Souza|\n",
      "|Belo Horizonte|   35| Pedro Santos|\n",
      "+--------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exibir os esquemas dos DataFrames\n",
    "print(\"Equema do DataFrame de vendas:\")\n",
    "df_vendas.printSchema()\n",
    "\n",
    "print(\"\\nEsquema do DataFrame de clientes:\")\n",
    "df_clientes.printSchema()\n",
    "\n",
    "#Exibição dos dados:\n",
    "print(\"Dados de vendas:\")\n",
    "df_vendas.show()\n",
    "\n",
    "print(\"\\nDados de clientes:\")\n",
    "df_clientes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec98977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vendas com quantidade maior que 3:\n",
      "+--------+----------+-------+-------+----------+--------------+-----------+\n",
      "|ID_Venda|Data_Venda|Cliente|Produto|Quantidade|Preco_Unitario|Valor_Total|\n",
      "+--------+----------+-------+-------+----------+--------------+-----------+\n",
      "+--------+----------+-------+-------+----------+--------------+-----------+\n",
      "\n",
      "\n",
      "Clientes de São Paulo\n",
      "+---------+-----+-------------+\n",
      "|   cidade|idade|         nome|\n",
      "+---------+-----+-------------+\n",
      "|São Paulo|   30|João da Silva|\n",
      "+---------+-----+-------------+\n",
      "\n",
      "Valor total das vendas:\n",
      "+-----------+\n",
      "|valor_total|\n",
      "+-----------+\n",
      "|      265.0|\n",
      "+-----------+\n",
      "\n",
      "\n",
      "Idade média dos clientes:\n",
      "+-----------+\n",
      "|idade_media|\n",
      "+-----------+\n",
      "|       30.0|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 7 columns and the second input has 4 columns. SQLSTATE: 42826;\n'Union false, false\n:- Relation [ID_Venda#430,Data_Venda#431,Cliente#432,Produto#433,Quantidade#434,Preco_Unitario#435,Valor_Total#436] csv\n+- LogicalRDD [id#832L, produto#833, quantidade#834L, Preco_Unitario#835], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 30\u001b[0m\n\u001b[1;32m     25\u001b[0m df_vendas_adicionais \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([\n\u001b[1;32m     26\u001b[0m     (\u001b[38;5;241m6\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonitor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m800.00\u001b[39m),\n\u001b[1;32m     27\u001b[0m     (\u001b[38;5;241m7\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeclado\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m100.00\u001b[39m)],[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantidade\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreco_Unitario\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#Unir os DataFrames de vendas\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m df_vendas_uniao \u001b[38;5;241m=\u001b[39m \u001b[43mdf_vendas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_vendas_adicionais\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnião dos DataFrames de vendas:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m df_vendas_adicionais\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py:1148\u001b[0m, in \u001b[0;36mDataFrame.union\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munion\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: ParentDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParentDataFrame:\n\u001b[0;32m-> 1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 7 columns and the second input has 4 columns. SQLSTATE: 42826;\n'Union false, false\n:- Relation [ID_Venda#430,Data_Venda#431,Cliente#432,Produto#433,Quantidade#434,Preco_Unitario#435,Valor_Total#436] csv\n+- LogicalRDD [id#832L, produto#833, quantidade#834L, Preco_Unitario#835], false\n"
     ]
    }
   ],
   "source": [
    "#Filtragem \n",
    "#Filtrar vendas com quantidade maior que 3\n",
    "vendas_filtradas = df_vendas.filter(col(\"quantidade\") > 3)\n",
    "print(\"Vendas com quantidade maior que 3:\")\n",
    "vendas_filtradas.show()\n",
    "\n",
    "#Filtrar clientes de São Paulo\n",
    "clientes_sp = df_clientes.filter(col(\"cidade\") == \"São Paulo\")\n",
    "print(\"\\nClientes de São Paulo\")\n",
    "clientes_sp.show()\n",
    "\n",
    "#Agregação\n",
    "#Calcular o valor total das vendas\n",
    "valor_total = df_vendas.agg(sum(\"Preco_Unitario\").alias(\"valor_total\"))\n",
    "print(\"Valor total das vendas:\")\n",
    "valor_total.show()\n",
    "\n",
    "#Calcular a média de idade dos clientes \n",
    "idade_media = df_clientes.agg(avg(\"idade\").alias(\"idade_media\"))\n",
    "print(\"\\nIdade média dos clientes:\")\n",
    "idade_media.show()\n",
    "\n",
    "#União\n",
    "#Criar um novo DataFrame com informações adicionais de vendas\n",
    "df_vendas_adicionais = spark.createDataFrame([\n",
    "    (6,\"Monitor\",2,800.00),\n",
    "    (7,\"Teclado\",8,100.00)],[\"ID_Venda\",\"produto\",\"quantidade\",\"Preco_Unitario\"])\n",
    "\n",
    "#Unir os DataFrames de vendas\n",
    "df_vendas_uniao = df_vendas.union(df_vendas_adicionais)\n",
    "print(\"União dos DataFrames de vendas:\")\n",
    "df_vendas_adicionais.show()\n",
    "#Escrita de dados:\n",
    "\n",
    "#Escrever o DataFrame de vendas unificado em um arquivo CSV\n",
    "df_vendas_uniao.write.csv(\"vendas_unificadas.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"Arquivos exportados com sucesso!\")\n",
    "#Encerramento da sessão Spark:\n",
    "#Encerrar a sessão Spark\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
